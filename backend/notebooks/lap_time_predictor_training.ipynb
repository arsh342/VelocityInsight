{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lap Time Predictor Training\n",
    "\n",
    "This notebook trains the Lap Time Predictor model using telemetry data from multiple tracks.\n",
    "We'll use Barber and COTA for training, and test on Indianapolis to validate generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../app')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "from data.loader import load_race_telemetry_wide, load_lap_times, segment_laps_by_time\n",
    "from data.features import calculate_lap_features, calculate_tire_degradation_features, create_ml_features\n",
    "from ml.models import LapTimePredictor\n",
    "\n",
    "# Set up paths\n",
    "dataset_root = Path(\"/Users/arsh/Developer/Projects/gr2025/dataset\")\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training Data (Barber + COTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from multiple tracks for training\n",
    "training_tracks = [\n",
    "    (\"barber\", \"R1\"),\n",
    "    (\"barber\", \"R2\"),\n",
    "    (\"cota\", \"R1\"),\n",
    "    (\"cota\", \"R2\"),\n",
    "]\n",
    "\n",
    "all_lap_features = []\n",
    "\n",
    "for track, race in training_tracks:\n",
    "    print(f\"Processing {track} {race}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load telemetry and lap data\n",
    "        df_telemetry = load_race_telemetry_wide(dataset_root, track, race)\n",
    "        start, end, lapt = load_lap_times(dataset_root, track, race)\n",
    "        \n",
    "        # Segment telemetry by laps\n",
    "        df_segmented = segment_laps_by_time(df_telemetry, start, end)\n",
    "        \n",
    "        # Calculate lap features\n",
    "        lap_features = calculate_lap_features(df_segmented)\n",
    "        lap_features['track'] = track\n",
    "        lap_features['race'] = race\n",
    "        \n",
    "        all_lap_features.append(lap_features)\n",
    "        print(f\"  - {len(lap_features)} laps processed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  - Error: {e}\")\n",
    "\n",
    "# Combine all training data\n",
    "df_training = pd.concat(all_lap_features, ignore_index=True)\n",
    "print(f\"\\nTotal training laps: {len(df_training)}\")\n",
    "print(f\"Vehicles: {df_training['vehicle_id'].nunique()}\")\n",
    "print(f\"Tracks: {df_training['track'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering & Tire Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tire degradation features\n",
    "df_training = calculate_tire_degradation_features(df_training)\n",
    "\n",
    "# Create ML-ready features\n",
    "df_features = create_ml_features(df_training, target_col=\"lap_duration_s\")\n",
    "\n",
    "print(f\"Feature matrix shape: {df_features.shape}\")\n",
    "print(f\"Features: {list(df_features.columns)}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_counts = df_features.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "else:\n",
    "    print(\"\\nNo missing values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Lap Time Predictor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "feature_cols = [col for col in df_features.columns \n",
    "                if col not in [\"vehicle_id\", \"lap_id\", \"lap_duration_s\"]]\n",
    "\n",
    "X = df_features[feature_cols]\n",
    "y = df_features[\"lap_duration_s\"]\n",
    "\n",
    "print(f\"Training features: {len(feature_cols)}\")\n",
    "print(f\"Training samples: {len(X)}\")\n",
    "\n",
    "# Train model\n",
    "predictor = LapTimePredictor()\n",
    "metrics = predictor.fit(df_features, target_col=\"lap_duration_s\")\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"MAE: {metrics['mae']:.3f} seconds\")\n",
    "print(f\"R²: {metrics['r2']:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "if predictor.model is not None:\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': predictor.feature_names,\n",
    "        'importance': predictor.model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test on Unseen Track (Indianapolis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Indianapolis data for testing\n",
    "print(\"Testing on Indianapolis Race 1...\")\n",
    "\n",
    "try:\n",
    "    df_test_telemetry = load_race_telemetry_wide(dataset_root, \"indianapolis\", \"R1\")\n",
    "    start_test, end_test, lapt_test = load_lap_times(dataset_root, \"indianapolis\", \"R1\")\n",
    "    \n",
    "    df_test_segmented = segment_laps_by_time(df_test_telemetry, start_test, end_test)\n",
    "    lap_features_test = calculate_lap_features(df_test_segmented)\n",
    "    lap_features_test['track'] = 'indianapolis'\n",
    "    lap_features_test['race'] = 'R1'\n",
    "    \n",
    "    df_test_features = calculate_tire_degradation_features(lap_features_test)\n",
    "    df_test_ml = create_ml_features(df_test_features, target_col=\"lap_duration_s\")\n",
    "    \n",
    "    # Make predictions\n",
    "    X_test = df_test_ml[feature_cols]\n",
    "    y_test = df_test_ml[\"lap_duration_s\"]\n",
    "    \n",
    "    predictions = predictor.predict(X_test)\n",
    "    \n",
    "    # Calculate test metrics\n",
    "    test_mae = mean_absolute_error(y_test, predictions)\n",
    "    test_r2 = r2_score(y_test, predictions)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    \n",
    "    print(f\"\\nTest Performance (Indianapolis):\")\n",
    "    print(f\"MAE: {test_mae:.3f} seconds\")\n",
    "    print(f\"R²: {test_r2:.3f}\")\n",
    "    print(f\"RMSE: {test_rmse:.3f} seconds\")\n",
    "    \n",
    "    # Visualize predictions vs actual\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_test, predictions, alpha=0.6)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Lap Time (s)')\n",
    "    plt.ylabel('Predicted Lap Time (s)')\n",
    "    plt.title(f'Predictions vs Actual\\nR² = {test_r2:.3f}')\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    residuals = predictions - y_test\n",
    "    plt.scatter(predictions, residuals, alpha=0.6)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Lap Time (s)')\n",
    "    plt.ylabel('Residuals (s)')\n",
    "    plt.title('Residual Plot')\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.hist(residuals, bins=20, alpha=0.7)\n",
    "    plt.xlabel('Residuals (s)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Residual Distribution')\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    # Show lap progression for one vehicle\n",
    "    vehicle_id = df_test_ml['vehicle_id'].iloc[0]\n",
    "    vehicle_data = df_test_ml[df_test_ml['vehicle_id'] == vehicle_id].sort_values('lap_id')\n",
    "    plt.plot(vehicle_data['lap_id'], vehicle_data['lap_duration_s'], 'o-', label='Actual')\n",
    "    vehicle_preds = predictions[df_test_ml['vehicle_id'] == vehicle_id]\n",
    "    plt.plot(vehicle_data['lap_id'], vehicle_preds, 's-', label='Predicted')\n",
    "    plt.xlabel('Lap Number')\n",
    "    plt.ylabel('Lap Time (s)')\n",
    "    plt.title(f'Lap Progression - {vehicle_id}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error testing on Indianapolis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Analysis & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance\n",
    "if predictor.model is not None:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Feature importance plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    top_features = importance_df.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 15 Feature Importances')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Performance by track\n",
    "    plt.subplot(2, 2, 2)\n",
    "    track_performance = df_training.groupby('track')['lap_duration_s'].agg(['mean', 'std'])\n",
    "    plt.bar(track_performance.index, track_performance['mean'], \n",
    "            yerr=track_performance['std'], capsize=5)\n",
    "    plt.xlabel('Track')\n",
    "    plt.ylabel('Average Lap Time (s)')\n",
    "    plt.title('Average Lap Times by Track')\n",
    "    \n",
    "    # Tire degradation analysis\n",
    "    plt.subplot(2, 2, 3)\n",
    "    if 'lap_progression' in df_training.columns:\n",
    "        degradation_sample = df_training[df_training['vehicle_id'] == df_training['vehicle_id'].iloc[0]]\n",
    "        plt.plot(degradation_sample['lap_progression'], degradation_sample['lap_duration_s'], 'o-')\n",
    "        plt.xlabel('Lap Number')\n",
    "        plt.ylabel('Lap Time (s)')\n",
    "        plt.title('Tire Degradation Example')\n",
    "    \n",
    "    # Speed vs lap time correlation\n",
    "    plt.subplot(2, 2, 4)\n",
    "    if 'avg_speed_kmh' in df_training.columns:\n",
    "        plt.scatter(df_training['avg_speed_kmh'], df_training['lap_duration_s'], alpha=0.5)\n",
    "        plt.xlabel('Average Speed (km/h)')\n",
    "        plt.ylabel('Lap Time (s)')\n",
    "        plt.title('Speed vs Lap Time')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nModel Training Complete!\")\n",
    "print(f\"The model can predict lap times with {metrics['mae']:.2f}s average error\")\n",
    "print(f\"and explains {metrics['r2']*100:.1f}% of lap time variance.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
